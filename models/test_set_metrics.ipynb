{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNet implementation for pet segmentation\n",
    "This module implements a UNet model that follows the architecture and hyperparameters\n",
    "defined by nnU-Net for the pet segmentation task.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Union, Optional, Type\n",
    "\n",
    "\n",
    "class SpatialDropout2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial dropout for 2D feature maps that drops entire channels.\n",
    "    This performs better than standard dropout for convolutional features.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super(SpatialDropout2d, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0:\n",
    "            return x\n",
    "            \n",
    "        # Get dimensions\n",
    "        _, channels, height, width = x.size()\n",
    "        \n",
    "        # Sample binary dropout mask\n",
    "        mask = x.new_empty(x.size(0), channels, 1, 1).bernoulli_(1 - self.drop_prob)\n",
    "        mask = mask.div_(1 - self.drop_prob)\n",
    "        \n",
    "        # Apply mask\n",
    "        x = x * mask.expand_as(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block for UNet with spatial dropout.\n",
    "    This block consists of n_convs convolutional layers, each followed by normalization, \n",
    "    activation, and optional spatial dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]],\n",
    "        stride: Union[int, Tuple[int, int]],\n",
    "        n_convs: int = 2,\n",
    "        padding: Optional[int] = None,\n",
    "        norm_op: Type[nn.Module] = nn.InstanceNorm2d,\n",
    "        norm_op_kwargs: Dict = None,\n",
    "        dropout_op: Optional[Type[nn.Module]] = None,\n",
    "        dropout_op_kwargs: Dict = None,\n",
    "        nonlin: Type[nn.Module] = nn.LeakyReLU,\n",
    "        nonlin_kwargs: Dict = None,\n",
    "        conv_bias: bool = True,\n",
    "        spatial_dropout_rate: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConvBlock.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            kernel_size: Size of the convolutional kernel\n",
    "            stride: Stride of the convolution\n",
    "            n_convs: Number of convolutional layers in the block\n",
    "            padding: Padding size (if None, calculated to maintain spatial dimensions)\n",
    "            norm_op: Normalization operation to use\n",
    "            norm_op_kwargs: Arguments for normalization operation\n",
    "            dropout_op: Dropout operation to use (if any)\n",
    "            dropout_op_kwargs: Arguments for dropout operation\n",
    "            nonlin: Non-linear activation function to use\n",
    "            nonlin_kwargs: Arguments for non-linear activation\n",
    "            conv_bias: Whether to use bias in convolutions\n",
    "            spatial_dropout_rate: Rate for spatial dropout (0 to disable)\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        # Default arguments if not provided\n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True}\n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'inplace': True}\n",
    "        if dropout_op_kwargs is None:\n",
    "            dropout_op_kwargs = {}\n",
    "        \n",
    "        # Calculate padding if not provided\n",
    "        if padding is None:\n",
    "            if isinstance(kernel_size, int):\n",
    "                padding = kernel_size // 2\n",
    "            else:\n",
    "                padding = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "        \n",
    "        # Create the convolutional blocks\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i in range(n_convs):\n",
    "            # Only apply stride in the first convolution\n",
    "            current_stride = stride if i == 0 else 1\n",
    "            \n",
    "            # Add convolutional layer\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    current_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    current_stride,\n",
    "                    padding,\n",
    "                    bias=conv_bias\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Add normalization\n",
    "            if norm_op is not None:\n",
    "                layers.append(norm_op(out_channels, **norm_op_kwargs))\n",
    "            \n",
    "            # Add non-linearity\n",
    "            if nonlin is not None:\n",
    "                layers.append(nonlin(**nonlin_kwargs))\n",
    "            \n",
    "            # Add spatial dropout if rate > 0\n",
    "            if spatial_dropout_rate > 0:\n",
    "                layers.append(SpatialDropout2d(spatial_dropout_rate))\n",
    "            \n",
    "            # Add regular dropout if specified\n",
    "            if dropout_op is not None:\n",
    "                layers.append(dropout_op(**dropout_op_kwargs))\n",
    "            \n",
    "            # Update current number of channels\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        # Create the sequential block\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the convolutional block.\"\"\"\n",
    "        return self.block(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block for the decoder part of UNet.\n",
    "    This block upsamples the feature maps and concatenates with skip connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        skip_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]],\n",
    "        n_convs: int = 2,\n",
    "        norm_op: Type[nn.Module] = nn.InstanceNorm2d,\n",
    "        norm_op_kwargs: Dict = None,\n",
    "        dropout_op: Optional[Type[nn.Module]] = None,\n",
    "        dropout_op_kwargs: Dict = None,\n",
    "        nonlin: Type[nn.Module] = nn.LeakyReLU,\n",
    "        nonlin_kwargs: Dict = None,\n",
    "        conv_bias: bool = True,\n",
    "        spatial_dropout_rate: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the UpBlock.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels from the lower level\n",
    "            skip_channels: Number of channels from the skip connection\n",
    "            out_channels: Number of output channels\n",
    "            kernel_size: Size of the convolutional kernel\n",
    "            n_convs: Number of convolutional layers in the block\n",
    "            norm_op: Normalization operation to use\n",
    "            norm_op_kwargs: Arguments for normalization operation\n",
    "            dropout_op: Dropout operation to use (if any)\n",
    "            dropout_op_kwargs: Arguments for dropout operation\n",
    "            nonlin: Non-linear activation function to use\n",
    "            nonlin_kwargs: Arguments for non-linear activation\n",
    "            conv_bias: Whether to use bias in convolutions\n",
    "            spatial_dropout_rate: Rate for spatial dropout (0 to disable)\n",
    "        \"\"\"\n",
    "        super(UpBlock, self).__init__()\n",
    "        \n",
    "        # Create the convolution block\n",
    "        self.conv_block = ConvBlock(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            n_convs=n_convs,\n",
    "            padding=None,\n",
    "            norm_op=norm_op,\n",
    "            norm_op_kwargs=norm_op_kwargs,\n",
    "            dropout_op=dropout_op,\n",
    "            dropout_op_kwargs=dropout_op_kwargs,\n",
    "            nonlin=nonlin,\n",
    "            nonlin_kwargs=nonlin_kwargs,\n",
    "            conv_bias=conv_bias,\n",
    "            spatial_dropout_rate=spatial_dropout_rate\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        \"\"\"\n",
    "        Forward pass of the upsampling block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input feature maps from lower level\n",
    "            skip: Feature maps from skip connection\n",
    "        \n",
    "        Returns:\n",
    "            Output feature maps\n",
    "        \"\"\"\n",
    "        # Upsample the input to match skip connection size\n",
    "        x_shape = x.shape[2:]\n",
    "        skip_shape = skip.shape[2:]\n",
    "        \n",
    "        # Compute upsampling size to match skip connection\n",
    "        if x_shape[0] != skip_shape[0] or x_shape[1] != skip_shape[1]:\n",
    "            x = F.interpolate(\n",
    "                x,\n",
    "                size=skip_shape,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        # Concatenate with skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        # Apply convolution block\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet model for semantic segmentation with original architecture and added spatial dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 3,\n",
    "        n_stages: int = 8,  # Original depth\n",
    "        features_per_stage: List[int] = None,\n",
    "        kernel_sizes: List[Tuple[int, int]] = None,\n",
    "        strides: List[Tuple[int, int]] = None,\n",
    "        n_conv_per_stage: List[int] = None,\n",
    "        n_conv_per_stage_decoder: List[int] = None,\n",
    "        conv_bias: bool = True,\n",
    "        norm_op: Type[nn.Module] = nn.InstanceNorm2d,\n",
    "        norm_op_kwargs: Dict = None,\n",
    "        dropout_op: Optional[Type[nn.Module]] = None,\n",
    "        dropout_op_kwargs: Dict = None,\n",
    "        nonlin: Type[nn.Module] = nn.LeakyReLU,\n",
    "        nonlin_kwargs: Dict = None,\n",
    "        encoder_dropout_rates: List[float] = None,\n",
    "        decoder_dropout_rates: List[float] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the UNet model.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels (3 for RGB images)\n",
    "            num_classes: Number of output classes (3 for background, cat, dog)\n",
    "            n_stages: Number of stages in the encoder\n",
    "            features_per_stage: Number of features per stage\n",
    "            kernel_sizes: Kernel sizes for each stage\n",
    "            strides: Strides for each stage\n",
    "            n_conv_per_stage: Number of convolutions per encoder stage\n",
    "            n_conv_per_stage_decoder: Number of convolutions per decoder stage\n",
    "            conv_bias: Whether to use bias in convolutions\n",
    "            norm_op: Normalization operation to use\n",
    "            norm_op_kwargs: Arguments for normalization operation\n",
    "            dropout_op: Dropout operation to use (if any)\n",
    "            dropout_op_kwargs: Arguments for dropout operation\n",
    "            nonlin: Non-linear activation function to use\n",
    "            nonlin_kwargs: Arguments for non-linear activation\n",
    "            encoder_dropout_rates: Dropout rates for each encoder stage\n",
    "            decoder_dropout_rates: Dropout rates for each decoder stage\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Set default values for parameters if not provided\n",
    "        if features_per_stage is None:\n",
    "            features_per_stage = [32, 64, 128, 256, 512, 512, 512, 512]  # Original architecture\n",
    "        \n",
    "        if kernel_sizes is None:\n",
    "            kernel_sizes = [[3, 3]] * n_stages\n",
    "        \n",
    "        if strides is None:\n",
    "            strides = [[1, 1]] + [[2, 2]] * (n_stages - 1)\n",
    "        \n",
    "        if n_conv_per_stage is None:\n",
    "            n_conv_per_stage = [2] * n_stages\n",
    "        \n",
    "        if n_conv_per_stage_decoder is None:\n",
    "            n_conv_per_stage_decoder = [2] * (n_stages - 1)\n",
    "        \n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True}\n",
    "        \n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'inplace': True}\n",
    "            \n",
    "        # Default dropout rates if not provided\n",
    "        if encoder_dropout_rates is None:\n",
    "            # Light dropout in encoder, increasing with depth\n",
    "            encoder_dropout_rates = [0.0, 0.0, 0.0, 0.1, 0.1, 0.2, 0.2, 0.3]\n",
    "            \n",
    "        if decoder_dropout_rates is None:\n",
    "            # Dropout in decoder, decreasing toward output\n",
    "            decoder_dropout_rates = [0.3, 0.2, 0.2, 0.1, 0.1, 0.0, 0.0]\n",
    "        \n",
    "        # Store parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.n_stages = n_stages\n",
    "        self.features_per_stage = features_per_stage\n",
    "        \n",
    "        # Create encoder stages\n",
    "        self.encoder_stages = nn.ModuleList()\n",
    "        \n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for stage in range(n_stages):\n",
    "            # Create encoder block\n",
    "            self.encoder_stages.append(\n",
    "                ConvBlock(\n",
    "                    current_channels,\n",
    "                    features_per_stage[stage],\n",
    "                    kernel_sizes[stage],\n",
    "                    strides[stage],\n",
    "                    n_convs=n_conv_per_stage[stage],\n",
    "                    norm_op=norm_op,\n",
    "                    norm_op_kwargs=norm_op_kwargs,\n",
    "                    dropout_op=dropout_op,\n",
    "                    dropout_op_kwargs=dropout_op_kwargs,\n",
    "                    nonlin=nonlin,\n",
    "                    nonlin_kwargs=nonlin_kwargs,\n",
    "                    conv_bias=conv_bias,\n",
    "                    spatial_dropout_rate=encoder_dropout_rates[stage]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Update current channels\n",
    "            current_channels = features_per_stage[stage]\n",
    "        \n",
    "        # Create decoder stages\n",
    "        self.decoder_stages = nn.ModuleList()\n",
    "        \n",
    "        for stage in range(n_stages - 1):\n",
    "            # Decoder stage goes in reverse order\n",
    "            decoder_idx = n_stages - 2 - stage\n",
    "            \n",
    "            # Create decoder block\n",
    "            self.decoder_stages.append(\n",
    "                UpBlock(\n",
    "                    features_per_stage[decoder_idx + 1],\n",
    "                    features_per_stage[decoder_idx],\n",
    "                    features_per_stage[decoder_idx],\n",
    "                    kernel_sizes[decoder_idx],\n",
    "                    n_convs=n_conv_per_stage_decoder[decoder_idx],\n",
    "                    norm_op=norm_op,\n",
    "                    norm_op_kwargs=norm_op_kwargs,\n",
    "                    dropout_op=dropout_op,\n",
    "                    dropout_op_kwargs=dropout_op_kwargs,\n",
    "                    nonlin=nonlin,\n",
    "                    nonlin_kwargs=nonlin_kwargs,\n",
    "                    conv_bias=conv_bias,\n",
    "                    spatial_dropout_rate=decoder_dropout_rates[stage]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Create final segmentation output layer\n",
    "        self.segmentation_output = nn.Conv2d(\n",
    "            features_per_stage[0],  # First decoder stage features\n",
    "            num_classes,           # Number of output classes\n",
    "            kernel_size=1,         # 1x1 convolution\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize the weights of the network.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.InstanceNorm2d):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the UNet model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, height, width)\n",
    "                \n",
    "        Returns:\n",
    "            output: Final output tensor\n",
    "        \"\"\"\n",
    "        # Store skip connections\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Encoder path\n",
    "        for stage in self.encoder_stages[:-1]:  # All but the last stage\n",
    "            x = stage(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        # Bottom stage (without skip connection)\n",
    "        x = self.encoder_stages[-1](x)\n",
    "        \n",
    "        # Decoder path\n",
    "        for idx, decoder_stage in enumerate(self.decoder_stages):\n",
    "            # Use the appropriate skip connection (in reverse order)\n",
    "            skip_idx = len(skip_connections) - 1 - idx\n",
    "            skip = skip_connections[skip_idx]\n",
    "            \n",
    "            # Decoder block\n",
    "            x = decoder_stage(x, skip)\n",
    "        \n",
    "        # Final 1x1 convolution to produce segmentation map\n",
    "        output = self.segmentation_output(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Script: train.py\n",
    "\n",
    "This script trains a UNet model for pet segmentation using the Oxford-IIIT Pet Dataset.\n",
    "It handles the training loop, validation, checkpointing, and logging.\n",
    "\n",
    "Example Usage:\n",
    "    python src/train.py --data_dir data/processed --output_dir models/unet_pet_segmentation\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import local modules\n",
    "\n",
    "class PetSegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset class for the Oxford-IIIT Pet segmentation dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir: str,\n",
    "        masks_dir: str,\n",
    "        include_augmented: bool = True,\n",
    "        target_size: Tuple[int, int] = (512, 512)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            images_dir: Directory containing images\n",
    "            masks_dir: Directory containing mask annotations\n",
    "            include_augmented: Whether to include augmented images (if available)\n",
    "            target_size: Target size for images and masks (height, width)\n",
    "        \"\"\"\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Get all image files from the directory\n",
    "        self.image_files = sorted(list(self.images_dir.glob(\"*.jpg\")))\n",
    "        \n",
    "        # Check for augmented data directory\n",
    "        if include_augmented and (self.images_dir.parent / \"augmented\" / \"images\").exists():\n",
    "            aug_images_dir = self.images_dir.parent / \"augmented\" / \"images\"\n",
    "            aug_masks_dir = self.images_dir.parent / \"augmented\" / \"masks\"\n",
    "            \n",
    "            # Add augmented images to the dataset\n",
    "            aug_image_files = sorted(list(aug_images_dir.glob(\"*.jpg\")))\n",
    "            self.aug_image_files = aug_image_files\n",
    "            self.aug_masks_dir = aug_masks_dir\n",
    "            \n",
    "            self.image_files.extend(aug_image_files)\n",
    "        else:\n",
    "            self.aug_image_files = []\n",
    "            self.aug_masks_dir = None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing image and mask tensors\n",
    "        \"\"\"\n",
    "        # Get image file path\n",
    "        img_path = self.image_files[idx]\n",
    "        \n",
    "        # Determine if this is an augmented image\n",
    "        is_augmented = img_path in self.aug_image_files if self.aug_image_files else False\n",
    "        \n",
    "        # Get corresponding mask file path\n",
    "        if is_augmented and self.aug_masks_dir:\n",
    "            mask_path = self.aug_masks_dir / f\"{img_path.stem}.png\"\n",
    "        else:\n",
    "            mask_path = self.masks_dir / f\"{img_path.stem}.png\"\n",
    "        \n",
    "        # Load image and mask\n",
    "        try:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
    "                \n",
    "            # Store original dimensions before resizing\n",
    "            original_dims = mask.shape[:2]  # (height, width)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image or mask: {e}\")\n",
    "            # Return a blank sample as fallback\n",
    "            image = np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8)\n",
    "            mask = np.zeros(self.target_size, dtype=np.uint8)\n",
    "            original_dims = self.target_size\n",
    "        \n",
    "        # Ensure image and mask have the correct dimensions\n",
    "        if image.shape[:2] != self.target_size:\n",
    "            image = cv2.resize(image, (self.target_size[1], self.target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        if mask.shape != self.target_size:\n",
    "            mask = cv2.resize(mask, (self.target_size[1], self.target_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Keep 255 as is - we'll handle it properly in the loss function\n",
    "        # Just ensure other values are within valid range (0, 1, 2)\n",
    "        mask = np.where((mask > 2) & (mask != 255), 0, mask)\n",
    "        \n",
    "        # Convert image to tensor and normalize (0-1)\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        # Apply standardization (approximately equivalent to ImageNet normalization)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        # Convert mask to tensor\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        # Store original dimensions as tensor\n",
    "        original_dims = torch.tensor(original_dims)\n",
    "        \n",
    "        return {\n",
    "            \"image\": image, \n",
    "            \"mask\": mask,\n",
    "            \"original_dims\": original_dims\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder_stages): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): SpatialDropout2d()\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (7): SpatialDropout2d()\n",
       "      )\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): SpatialDropout2d()\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (7): SpatialDropout2d()\n",
       "      )\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): SpatialDropout2d()\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (7): SpatialDropout2d()\n",
       "      )\n",
       "    )\n",
       "    (5): ConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (3): SpatialDropout2d()\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (7): SpatialDropout2d()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_stages): ModuleList(\n",
       "    (0): UpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): SpatialDropout2d()\n",
       "          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (7): SpatialDropout2d()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): SpatialDropout2d()\n",
       "          (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (7): SpatialDropout2d()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): SpatialDropout2d()\n",
       "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (7): SpatialDropout2d()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): SpatialDropout2d()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (7): SpatialDropout2d()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_output): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ðŸ“¦ Imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ðŸ§  Load best model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    num_classes=3,\n",
    "    n_stages=6,\n",
    "    features_per_stage=[32, 64, 128, 256, 512, 512],\n",
    "    kernel_sizes=[[3, 3]] * 6,\n",
    "    strides=[[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]],\n",
    "    n_conv_per_stage=[2] * 6,\n",
    "    n_conv_per_stage_decoder=[2] * 5,\n",
    "    conv_bias=True,\n",
    "    norm_op=torch.nn.InstanceNorm2d,\n",
    "    norm_op_kwargs={\"eps\": 1e-5, \"affine\": True},\n",
    "    dropout_op=None,\n",
    "    nonlin=torch.nn.LeakyReLU,\n",
    "    nonlin_kwargs={\"inplace\": True},\n",
    "    encoder_dropout_rates=[0.0, 0.0, 0.1, 0.2, 0.3, 0.3],\n",
    "    decoder_dropout_rates=[0.3, 0.2, 0.2, 0.1, 0.0]\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/ulixes/segmentation_cv/unet/models/unet_pet_segmentation_reduced/best_model.pth\",\n",
    "    map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")[\"model_state_dict\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = PetSegmentationDataset(\n",
    "    images_dir=\"/home/ulixes/segmentation_cv/unet/data/processed/Test/resized\",\n",
    "    masks_dir=\"/home/ulixes/segmentation_cv/unet/data/processed/Test/processed_labels\",\n",
    "    include_augmented=False,\n",
    "    target_size=(512, 512)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, ignore_index=255):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Initialize accumulators for each class\n",
    "        self.intersections = np.zeros(self.num_classes)\n",
    "        self.unions = np.zeros(self.num_classes)\n",
    "        self.true_positives = np.zeros(self.num_classes)\n",
    "        self.false_positives = np.zeros(self.num_classes)\n",
    "        self.false_negatives = np.zeros(self.num_classes)\n",
    "        self.total_pixels = 0\n",
    "        self.correct_pixels = 0\n",
    "        \n",
    "    def update(self, pred, target):\n",
    "        \"\"\"\n",
    "        Update accumulators with a new batch of predictions and targets\n",
    "        \n",
    "        Args:\n",
    "            pred: prediction tensor/array\n",
    "            target: ground truth tensor/array\n",
    "        \"\"\"\n",
    "        mask = (target != self.ignore_index)\n",
    "        self.total_pixels += mask.sum()\n",
    "        self.correct_pixels += ((pred == target) & mask).sum()\n",
    "        \n",
    "        # Update class-wise metrics\n",
    "        for cls in range(self.num_classes):\n",
    "            pred_cls = (pred == cls) & mask\n",
    "            target_cls = (target == cls) & mask\n",
    "            \n",
    "            intersection = (pred_cls & target_cls).sum()\n",
    "            union = (pred_cls | target_cls).sum()\n",
    "            \n",
    "            # Update accumulators\n",
    "            self.intersections[cls] += intersection\n",
    "            self.unions[cls] += union\n",
    "            \n",
    "            # For dice coefficient\n",
    "            self.true_positives[cls] += intersection\n",
    "            self.false_positives[cls] += pred_cls.sum() - intersection\n",
    "            self.false_negatives[cls] += target_cls.sum() - intersection\n",
    "            \n",
    "    def compute_dice(self, cls):\n",
    "        \"\"\"\n",
    "        Compute Dice coefficient for a specific class using accumulated statistics\n",
    "        \n",
    "        Args:\n",
    "            cls: class index\n",
    "            \n",
    "        Returns:\n",
    "            dice: Dice coefficient for the specified class\n",
    "        \"\"\"\n",
    "        numerator = 2 * self.true_positives[cls]\n",
    "        denominator = 2 * self.true_positives[cls] + self.false_positives[cls] + self.false_negatives[cls]\n",
    "        \n",
    "        if denominator > 0:\n",
    "            return (numerator / denominator).item()\n",
    "        return float('nan')\n",
    "    \n",
    "    def compute_pixel_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute overall pixel accuracy using accumulated statistics\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: Pixel accuracy across the entire dataset\n",
    "        \"\"\"\n",
    "        if self.total_pixels > 0:\n",
    "            return (self.correct_pixels / self.total_pixels).item()\n",
    "        return float('nan')\n",
    "    \n",
    "    def compute_iou(self, cls):\n",
    "        \"\"\"\n",
    "        Compute IoU for a specific class using accumulated statistics\n",
    "        \n",
    "        Args:\n",
    "            cls: class index\n",
    "            \n",
    "        Returns:\n",
    "            iou: IoU for the specified class\n",
    "        \"\"\"\n",
    "        if self.unions[cls] > 0:\n",
    "            return (self.intersections[cls] / self.unions[cls]).item()\n",
    "        return float('nan')\n",
    "    \n",
    "    def compute_mean_iou(self):\n",
    "        \"\"\"\n",
    "        Compute mean IoU across all classes\n",
    "        \n",
    "        Returns:\n",
    "            mean_iou: Mean IoU value\n",
    "        \"\"\"\n",
    "        valid_ious = []\n",
    "        for cls in range(self.num_classes):\n",
    "            iou = self.compute_iou(cls)\n",
    "            if not np.isnan(iou):\n",
    "                valid_ious.append(iou)\n",
    "        \n",
    "        if valid_ious:\n",
    "            return sum(valid_ious) / len(valid_ious)\n",
    "        return float('nan')\n",
    "\n",
    "# Functions that maintain the same API as original but use the accumulation approach\n",
    "def compute_dice(pred, target, cls, ignore_index=255):\n",
    "    \"\"\"\n",
    "    This function should be used for a single prediction-target pair.\n",
    "    For dataset-level metrics, use SegmentationMetrics class.\n",
    "    \"\"\"\n",
    "    metrics = SegmentationMetrics(num_classes=cls+1, ignore_index=ignore_index)\n",
    "    metrics.update(pred, target)\n",
    "    return metrics.compute_dice(cls)\n",
    "\n",
    "def compute_pixel_accuracy(pred, target, ignore_index=255):\n",
    "    \"\"\"\n",
    "    This function should be used for a single prediction-target pair.\n",
    "    For dataset-level metrics, use SegmentationMetrics class.\n",
    "    \"\"\"\n",
    "    metrics = SegmentationMetrics(num_classes=max(pred.max(), target.max())+1, ignore_index=ignore_index)\n",
    "    metrics.update(pred, target)\n",
    "    return metrics.compute_pixel_accuracy()\n",
    "\n",
    "def compute_iou(pred, target, cls, ignore_index=255):\n",
    "    \"\"\"\n",
    "    This function should be used for a single prediction-target pair.\n",
    "    For dataset-level metrics, use SegmentationMetrics class.\n",
    "    \"\"\"\n",
    "    metrics = SegmentationMetrics(num_classes=cls+1, ignore_index=ignore_index)\n",
    "    metrics.update(pred, target)\n",
    "    return metrics.compute_iou(cls)\n",
    "\n",
    "# Example usage:\n",
    "# For dataset-level metrics:\n",
    "# metrics = SegmentationMetrics(num_classes=3)\n",
    "# for pred, target in dataset:\n",
    "#     metrics.update(pred, target)\n",
    "# \n",
    "# # Get metrics after processing all images\n",
    "# mean_iou = metrics.compute_mean_iou()\n",
    "# class_0_iou = metrics.compute_iou(0)\n",
    "# pixel_accuracy = metrics.compute_pixel_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples found: 3694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test samples found: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 924/924 [00:27<00:00, 33.14it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = SegmentationMetrics(num_classes=3, ignore_index=255)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        masks = batch[\"mask\"].to(device)\n",
    "        original_dims = batch[\"original_dims\"]\n",
    "\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        for i in range(preds.size(0)):\n",
    "            orig_h, orig_w = original_dims[i]\n",
    "            pred_resized = F.interpolate(\n",
    "                preds[i][None, None].float(),\n",
    "                size=(orig_h, orig_w),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze().long()\n",
    "\n",
    "            mask_resized = F.interpolate(\n",
    "                masks[i][None, None].float(),\n",
    "                size=(orig_h, orig_w),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze().long()\n",
    "\n",
    "            # print(f\"Original mask shape: {masks[i].shape}\")\n",
    "            # print(f\"Target dimensions: {orig_h}x{orig_w}\")\n",
    "            # print(f\"Resized pred shape: {pred_resized.shape}\")\n",
    "            # print(f\"Resized mask shape: {mask_resized.shape}\")\n",
    "\n",
    "            metrics.update(pred_resized.cpu().numpy(), mask_resized.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Accuracy:       0.8757\n",
      "Dice (Background):    0.9260\n",
      "Dice (Cat):           0.7207\n",
      "Dice (Dog):           0.7816\n",
      "Mean Foreground Dice: 0.7511\n",
      "IoU (Background):     0.8622\n",
      "IoU (Cat):            0.5633\n",
      "IoU (Dog):            0.6414\n",
      "Mean IoU:             0.6890\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pixel Accuracy:       {metrics.compute_pixel_accuracy():.4f}\")\n",
    "print(f\"Dice (Background):    {metrics.compute_dice(0):.4f}\")\n",
    "print(f\"Dice (Cat):           {metrics.compute_dice(1):.4f}\")\n",
    "print(f\"Dice (Dog):           {metrics.compute_dice(2):.4f}\")\n",
    "print(f\"Mean Foreground Dice: {np.nanmean([metrics.compute_dice(1), metrics.compute_dice(2)]):.4f}\")\n",
    "print(f\"IoU (Background):     {metrics.compute_iou(0):.4f}\")\n",
    "print(f\"IoU (Cat):            {metrics.compute_iou(1):.4f}\")\n",
    "print(f\"IoU (Dog):            {metrics.compute_iou(2):.4f}\")\n",
    "print(f\"Mean IoU:             {metrics.compute_mean_iou():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
